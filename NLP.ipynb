{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Computational Linguistics and how does it relate to NLP?\n",
        "\n",
        "Answer:\n",
        "Computational Linguistics is a field that focuses on building computational models that understand, analyze, and generate human language in a way similar to humans. It combines elements of linguistics, artificial intelligence, and computer science to study language structure, meaning, and communication.\n",
        "\n",
        "Natural Language Processing (NLP) is a practical application area of computational linguistics. While computational linguistics develops theories about language understanding, NLP applies these theories to real-world tasks such as text classification, machine translation, speech recognition, and sentiment analysis. In short, Computational Linguistics provides the science, and NLP delivers the technology."
      ],
      "metadata": {
        "id": "Uq0uW34rYBON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Briefly describe the historical evolution of Natural Language Processing.\n",
        "\n",
        "Answer:\n",
        "The history of NLP can be described in four major phases:\n",
        "\n",
        "1950sâ€“1960s (Rule-Based Systems):\n",
        "Early NLP systems depended on hand-crafted grammar rules. Machine translation was the primary focus.\n",
        "\n",
        "1970sâ€“1980s (Statistical Linguistics):\n",
        "Introduction of probabilistic models and Hidden Markov Models enabled improved speech recognition and parsing.\n",
        "\n",
        "1990sâ€“2010 (Machine Learning Era):\n",
        "Supervised ML algorithms such as SVM, NaÃ¯ve Bayes, and decision trees became popular. Feature engineering played a central role.\n",
        "\n",
        "2011â€“Present (Deep Learning & Transformers):\n",
        "Use of neural networks, especially RNNs, LSTMs, and now Transformer-based models like BERT and GPT, achieved state-of-the-art performance in nearly all NLP tasks."
      ],
      "metadata": {
        "id": "vrgdC6kdYGQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: List and explain three major use cases of NLP in todayâ€™s tech industry.\n",
        "\n",
        "Answer:\n",
        "\n",
        "Chatbots & Virtual Assistants\n",
        "Used in customer support to understand user queries and respond automatically (e.g., Alexa, Siri, WhatsApp bots).\n",
        "\n",
        "Sentiment Analysis\n",
        "Analyzes customer reviews, social media posts, and feedback to determine opinions and emotions.\n",
        "\n",
        "Machine Translation\n",
        "Converts text from one language to another (e.g., Google Translate), enabling global communication and localization."
      ],
      "metadata": {
        "id": "wa9nAI3XYSDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is text normalization and why is it essential in text processing tasks?\n",
        "\n",
        "Answer:\n",
        "Text normalization is the process of transforming text into a standard, consistent format before analysis. It includes operations such as:\n",
        "\n",
        "Lowercasing\n",
        "\n",
        "Removing punctuation/special symbols\n",
        "\n",
        "Expanding contractions (e.g., â€œdonâ€™t â†’ do notâ€)\n",
        "\n",
        "Correcting spelling variations\n",
        "\n",
        "It is essential because raw text often contains noise. Normalization reduces variations, improves token matching, and increases the accuracy of NLP models."
      ],
      "metadata": {
        "id": "-B5qmU76YcDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare and contrast stemming and lemmatization with suitable examples.\n",
        "\n",
        "| Feature    | Stemming                                      | Lemmatization                                      |\n",
        "| ---------- | --------------------------------------------- | -------------------------------------------------- |\n",
        "| Definition | Removes suffixes to reduce words to root form | Converts words to their meaningful dictionary form |\n",
        "| Output     | May produce non-real words                    | Always produces valid words                        |\n",
        "| Technique  | Rule-based trimming                           | Uses vocabulary + morphological analysis           |\n",
        "| Speed      | Faster                                        | Slower but more accurate                           |\n",
        "\n",
        "Examples:\n",
        "\n",
        "Word: â€œstudiesâ€ â†’\n",
        "\n",
        "Stem: studi\n",
        "\n",
        "Lemma: study\n",
        "\n",
        "Word: â€œbetterâ€ â†’\n",
        "\n",
        "Stem: bett\n",
        "\n",
        "Lemma: good"
      ],
      "metadata": {
        "id": "8OIteCOPYdKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6: Python program to extract email addresses using Regex\n",
        "import re\n",
        "\n",
        "text = \"\"\"Hello team, please contact us at support@xyz.com for technical issues, or reach out to\n",
        "our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny\n",
        "via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.\"\"\"\n",
        "\n",
        "# Regex for email extraction\n",
        "emails = re.findall(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}', text)\n",
        "\n",
        "print(\"Extracted Emails:\")\n",
        "for email in emails:\n",
        "    print(email)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hP-BpM8fYtA2",
        "outputId": "7488fb0b-a98f-4209-a320-ca64b76c522e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Emails:\n",
            "support@xyz.com\n",
            "hr@xyz.com\n",
            "john.doe@xyz.org\n",
            "jenny_clarke126@mail.co.us\n",
            "partners@xyz.biz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Tokenization & Frequency Distribution using NLTK\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "nltk.download('punkt') # Download the punkt tokenizer models\n",
        "nltk.download('punkt_tab') # Download the punkt_tab resource as suggested by the error\n",
        "\n",
        "text = \"\"\"Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.\"\"\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Frequency Distribution\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"\\nTop 10 Most Frequent Words:\")\n",
        "print(freq_dist.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C_yCXmeY4nP",
        "outputId": "fc1b51a7-6daf-4714-e087-c24cb679d27d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'It', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Applications', 'of', 'NLP', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'machine', 'translation', '.', 'As', 'technology', 'advances', ',', 'the', 'role', 'of', 'NLP', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical', '.']\n",
            "\n",
            "Top 10 Most Frequent Words:\n",
            "[(',', 7), ('.', 4), ('NLP', 3), ('and', 3), ('is', 2), ('of', 2), ('Natural', 1), ('Language', 1), ('Processing', 1), ('(', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Custom Annotator for Proper Nouns using spaCy\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"John lives in London and works at Google.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    if token.pos_ == \"PROPN\":  # Proper Noun\n",
        "        print(token.text, \"â†’ Proper Noun\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqemRicAZS4Z",
        "outputId": "060bc819-0bdf-4403-a7d5-5a482266c0f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "John â†’ Proper Noun\n",
            "London â†’ Proper Noun\n",
            "Google â†’ Proper Noun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Word2Vec Model Training using Gensim\n",
        "!pip install gensim # Install the gensim library\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "dataset = [\n",
        "    \"Natural language processing enables computers to understand human language\",\n",
        "    \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        "    \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "    \"Text preprocessing is a critical step before training word embeddings\",\n",
        "    \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "# Tokenization & Lowercasing\n",
        "tokenized_data = [word_tokenize(sentence.lower()) for sentence in dataset]\n",
        "\n",
        "# Train Word2Vec Model\n",
        "model = Word2Vec(sentences=tokenized_data, vector_size=50, window=3, min_count=1, workers=4)\n",
        "\n",
        "print(\"Vector for word 'language':\")\n",
        "print(model.wv['language'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV_XfkOSZgB9",
        "outputId": "943ff512-30f6-4259-ccca-2ebfb643a4c3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Vector for word 'language':\n",
            "[-0.01427803  0.00248206 -0.01435343 -0.00448924  0.00743861  0.01166625\n",
            "  0.00239637  0.00420546 -0.00822078  0.01445067 -0.01261408  0.00929443\n",
            " -0.01643995  0.00407294 -0.0099541  -0.00849538 -0.00621797  0.01131042\n",
            "  0.0115968  -0.0099493   0.00154666 -0.01699156  0.01561961  0.01851458\n",
            " -0.00548466  0.00160045  0.0014933   0.01095577 -0.01721216  0.00116891\n",
            "  0.01373884  0.00446319  0.00224935 -0.01864431  0.01696473 -0.01252825\n",
            " -0.00598475  0.00698757 -0.00154526  0.00282258  0.00356398 -0.0136578\n",
            " -0.01944962  0.01808117  0.01239611 -0.01382586  0.00680696  0.00041213\n",
            "  0.00950749 -0.01423989]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Steps to analyze customer feedback using NLP\n",
        "\n",
        "Answer:\n",
        "\n",
        "ğŸ§  Step-by-Step Workflow\n",
        "\n",
        "Data Collection â€“ Load reviews from database/CSV.\n",
        "\n",
        "Text Cleaning â€“ Remove HTML tags, URLs, emojis, stopwords.\n",
        "\n",
        "Text Normalization â€“ Lowercase, stemming/lemmatization.\n",
        "\n",
        "Tokenization â€“ Split into words/sentences.\n",
        "\n",
        "Sentiment Analysis â€“ Identify positive/negative reviews.\n",
        "\n",
        "Topic Modeling â€“ Find common themes using LDA.\n",
        "\n",
        "Visualization â€“ Word clouds, bar charts for insights.\n",
        "\n",
        "Generate Actionable Insights â€“ Recommend improvements."
      ],
      "metadata": {
        "id": "JGSPq5PzZ0j_"
      }
    }
  ]
}